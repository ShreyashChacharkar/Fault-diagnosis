{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import important liberies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#visualisation liberies\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "#.rdata file reader\n",
    "import pyreadr\n",
    "\n",
    "#zipfile extract\n",
    "import zipfile as zp\n",
    "\n",
    "#sklearn\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "\n",
    "# # Import the required libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "wrangle() got an unexpected keyword argument 'drop'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m     reduced_data \u001b[39m=\u001b[39m reduced_data[reduced_data[\u001b[39m'\u001b[39m\u001b[39mfaultNumber\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m15\u001b[39m]\n\u001b[0;32m     10\u001b[0m     \u001b[39mreturn\u001b[39;00m reduced_data\n\u001b[1;32m---> 12\u001b[0m reu_df \u001b[39m=\u001b[39m wrangle(df,drop\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,simulation\u001b[39m=\u001b[39m[\u001b[39m1\u001b[39m,\u001b[39m20\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: wrangle() got an unexpected keyword argument 'drop'"
     ]
    }
   ],
   "source": [
    "df_ff = pd.read_csv(\"dataset/fault_free_training.csv\",index_col=\"Unnamed: 0\")\n",
    "df_faulty = pd.read_csv(\"dataset/faulty_training.csv\",index_col=\"Unnamed: 0\")\n",
    "df = pd.concat([df_faulty,df_ff])\n",
    "\n",
    "def wrangle(df, simulation):\n",
    "    reduced_data =df[(df['simulationRun'] >simulation[0] ) & (df['simulationRun'] <simulation[1] ) & (df['sample']>20)]\n",
    "    reduced_data = reduced_data[reduced_data['faultNumber'] != 3]\n",
    "    reduced_data = reduced_data[reduced_data['faultNumber'] != 9]\n",
    "    reduced_data = reduced_data[reduced_data['faultNumber'] != 15]\n",
    "    return reduced_data\n",
    "\n",
    "reu_df = wrangle(df,drop=None,simulation=[1,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "sc.fit(reu_df.iloc[:,3:])\n",
    "\n",
    "X = sc.transform(reu_df.iloc[:,3:])\n",
    "Y = reu_df['faultNumber']\n",
    "le = LabelEncoder()\n",
    "Y_enc = le.fit_transform(Y)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y_enc, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainable AI\n",
    "## Local\n",
    "### Lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "lime_explainer = lime.lime_tabular.LimeTabularExplainer(x_train, feature_names=reu_df.columns[3:], training_labels= y_train, discretize_continuous=True)\n",
    "idx=1\n",
    "y_pred = xg.predict(np.expand_dims(x_test[idx], axis=0))\n",
    "# y_pred = enc.inverse_transform(y_pred)[0][0]\n",
    "print(f\"Predicted fault is {y_pred}\")\n",
    "exp = lime_explainer.explain_instance(x_test[idx], xg.predict_proba, num_features=10, top_labels=1)\n",
    "note = exp.show_in_notebook(show_table=True, show_all=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import shap\n",
    "shap.initjs()\n",
    "x_train = pd.DataFrame(x_train, columns=df.iloc[:,3:].columns)\n",
    "x_test = pd.DataFrame(x_test, columns=df.iloc[:,3:].columns)\n",
    "\n",
    "x_train\n",
    "x_train[::50]\n",
    "# explain the model's predictions using SHAP\n",
    "explainer = shap.KernelExplainer(xg.predict_proba, x_train[::100])\n",
    "\n",
    "idx_sample = 34\n",
    "shap_values = explainer.shap_values(x_test[idx_sample])\n",
    "sampled_test_data = x_test[idx_sample]\n",
    "sampled_test_labels = y_test[idx_sample]\n",
    "\n",
    "print(f\"the predicted class is: {xg.predict([sampled_test_data])}\")\n",
    "cls_index =  sampled_test_labels\n",
    "print(f\"the Actual class is: {cls_index}\")\n",
    "\n",
    "for fault_cls in range(0,18):\n",
    "    print(f\"Force plot for fault class: {fault_cls}\")\n",
    "    display(shap.force_plot(explainer.expected_value[fault_cls], shap_values[fault_cls],sampled_test_data ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree explainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain the model's predictions using SHAP\n",
    "explainer = shap.TreeExplainer(xg)\n",
    "\n",
    "\n",
    "shap_values = explainer.shap_values(x_test[::50])\n",
    "# ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
    "# x_test[::50]\n",
    "# Visualize the SHAP values for a single example\n",
    "sampled_test_data = x_test[::50]\n",
    "sampled_test_labels = y_test[::50]\n",
    "\n",
    "sample_idx = 45\n",
    "\n",
    "print(f\"the predicted class is: {xg.predict(sampled_test_data[[sample_idx]])}\")\n",
    "cls_index =  sampled_test_labels[sample_idx]\n",
    "print(f\"the Actual class is: {cls_index}\")\n",
    "\n",
    "for fault_cls in range(0,18):\n",
    "    print(f\"Force plot for fault class: {fault_cls}\")\n",
    "    display(shap.force_plot(explainer.expected_value[fault_cls], shap_values[fault_cls][sample_idx],sampled_test_data[sample_idx],feature_names=reu_df.columns[3:]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(xg)\n",
    "\n",
    "F_num=10\n",
    "\n",
    "df_new= df[(df['faultNumber']==F_num) & (df['simulationRun']==1)].copy()\n",
    "x_new = df_new.iloc[:,3:]\n",
    "x_new = sc.transform(x_new)\n",
    "x_new = pd.DataFrame(x_new, columns=df_new.iloc[:,3:].columns)\n",
    "\n",
    "\n",
    "# # Generate random numbers and append to x_new\n",
    "# rand_nums = np.random.rand(len(x_new), 1)\n",
    "# x_new = np.hstack((x_new, rand_nums))\n",
    "\n",
    "# generate shapely values\n",
    "shap_values_sample = explainer.shap_values(x_new)\n",
    "\n",
    "\n",
    "y_pred = xg.predict(x_new)\n",
    "# shap values for this fault class\n",
    "data = shap_values_sample[le.transform([F_num])[0]]\n",
    "\n",
    "y_pred = le.inverse_transform(y_pred)\n",
    "\n",
    "\n",
    "f, ax = plt.subplots(figsize=(20,10))\n",
    "\n",
    "sns.scatterplot(x=df_new['sample'],y=y_pred,palette=\"coolwarm\",s=80,alpha=0.4)\n",
    "\n",
    "plt.axvline(20,c='r',alpha=0.8,linestyle ='--')\n",
    "plt.axhline(F_num,c='black',alpha=0.8,label='Actual Fault',linestyle ='--')\n",
    "plt.legend(bbox_to_anchor=(1.5, 1.05))\n",
    "plt.ylabel('Predicted fault class')\n",
    "plt.title(f'Actual fault-{F_num}')\n",
    "plt.yticks(np.arange(-1,21,1))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saliancy Feature\n",
    "## Global Feature importance\n",
    "### Premutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "from sklearn.inspection import permutation_importance\n",
    "result = permutation_importance(xg, x_test[::50], y_test[::50], n_repeats=10, random_state=42)\n",
    "\n",
    "# Get feature importances and standard deviations\n",
    "importances = result.importances_mean\n",
    "std_devs = result.importances_std\n",
    "# Print feature importances\n",
    "# for feature, importance, std_dev in zip(reu_df.iloc[:,3:].columns, importances, std_devs):\n",
    "#     print(f\"{feature}: {importance:.4f} ± {std_dev:.4f}\")\n",
    "\n",
    "res = pd.DataFrame(importances, columns=[\"Importances\"])\n",
    "res[\"std_dev\"] = std_devs\n",
    "res[\"features\"] = reu_df.iloc[:,3:].columns\n",
    "res[\"permutation_importance\"] = res.Importances.round(5).astype(\"str\") + \"±\" + res.std_dev.round(5).astype(\"str\")\n",
    "res.sort_values(\"Importances\",inplace=True,ascending=False)\n",
    "# str(res.Importances) + \"±\" + str(res.std_dev)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(res['features'].head(10), res['Importances'].head(10), yerr=res['std_dev'].head(10), capsize=5, color='skyblue')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importances')\n",
    "plt.title('Top 10 Features Importance with Error Bars')\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial dependence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "feat_name = 'xmv_10'\n",
    "PartialDependenceDisplay.from_estimator(xg, x_test[::50],features=[10], feature_names=reu_df.columns[3:],target=17)\n",
    "plt.show()\n",
    "### Shap\n",
    "fig = shap.summary_plot(shap_values,feature_names=reu_df.iloc[:,3:].columns, max_display=15)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[1], x_test[::50], max_display=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"xmeas_11\", shap_values[1], x_test[::50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'data' is a NumPy array or DataFrame with your data\n",
    "# Assuming 'sampled_test_data' is a DataFrame with column names\n",
    "\n",
    "# Create a figure\n",
    "fig = px.line()\n",
    "# Loop over the columns and add traces\n",
    "for i in range(data.shape[1]):\n",
    "    fig.add_trace(px.line(x=np.arange(data.shape[0]), y=data[:, i], name=f'{sampled_test_data.columns[i]}').data[0])\n",
    "\n",
    "# Set x-axis label and tick positions\n",
    "fig.update_xaxes(title_text='Sample', tickvals=np.arange(0, data.shape[0], 50))\n",
    "\n",
    "# Set y-axis label\n",
    "fig.update_yaxes(title_text='SHAP Value')\n",
    "\n",
    "# Add a legend\n",
    "fig.update_layout(legend=dict(x=1.05, y=1))\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create a figure and axis object\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "# loop over the columns and plot each one\n",
    "for i in range(data.shape[1]):\n",
    "    ax.plot(data[:, i], label=f'{sampled_test_data.columns[i]}')\n",
    "\n",
    "# set the x-axis label and tick positions\n",
    "ax.set_xlabel('Sample')\n",
    "ax.set_xticks(np.arange(0, data.shape[0], 50))\n",
    "\n",
    "# set the y-axis label\n",
    "ax.set_ylabel('SHAP Value')\n",
    "\n",
    "# add a legend\n",
    "ax.legend(bbox_to_anchor=(1.5, 1.05))\n",
    "# show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import shap\n",
    "import plotly.express as px\n",
    "\n",
    "# Assuming you've already calculated SHAP values as shap_values\n",
    "# and have the feature names as feature_names\n",
    "\n",
    "# Create a SHAP summary plot using Plotly Express\n",
    "fig = px.scatter(\n",
    "    x=shap_values[0],  # SHAP values for the first instance\n",
    "    y=df.iloc[:,3:].columns,  # Feature names\n",
    "    color=shap_values[0],  # Color by SHAP values\n",
    "    labels={\"x\": \"SHAP Value\"},\n",
    "    title=\"SHAP Summary Plot\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "_ = tree.plot_tree(dt, \n",
    "                   feature_names=reu_df.columns[3:],\n",
    "                   filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From above analysis, XGB and Random forest Performs well.\n",
    "### Metrics:\n",
    "\n",
    "In this series, we will use accuracy as the primary metric for evaluating the performance of the different machine learning algorithms. We will update the table as we evaluate the performance of other algorithms in the subsequently.\n",
    "\n",
    "\n",
    "Average accuracy score obtained for each method, excluding fault No. 9 and 15 (**No feature were Dropped, all 52 sensor measurements were used**)\n",
    "\n",
    "| Method                                    |Accuracy  |\n",
    "|-----------------------------------------  |----------|\n",
    "| XG Boost                                  |  0.924  |\n",
    "| Random Forest                           |  0.895   |\n",
    "| Naive Bayes                                   |  0.652   |\n",
    "| KNN                               |  0.464   |\n",
    "| Decision Tree                                  |  0.827   |\n",
    "| Logistic Regression                                  |  0.695   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# communicating result\n",
    "## For Non-technical\n",
    "### Real time Fault Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for F_num in range(0,21):\n",
    "    df_new= df[(df['faultNumber']==F_num) & (df['simulationRun']==1)].copy()\n",
    "    x_new = df_new.iloc[:,3:]\n",
    "    x_new = sc.transform(x_new)\n",
    "    y_pred = xg.predict(x_new)\n",
    "    y_pred = le.inverse_transform(y_pred)\n",
    "    y_proba = np.max(xg.predict_proba(x_new),axis=1)\n",
    "\n",
    "    f, ax = plt.subplots(figsize=(10,3))\n",
    "    sns.scatterplot(x=df_new['sample'],y=y_pred,hue=y_proba,palette=\"coolwarm\",s=80,alpha=0.4)\n",
    "    plt.axvline(20,c='r',alpha=0.8,linestyle ='--')\n",
    "    plt.axhline(F_num,c='cyan',alpha=0.8,label='Actual Fault',linestyle ='--')\n",
    "    plt.legend(bbox_to_anchor=(1.5, 1.05))\n",
    "    plt.ylabel('Predicted fault class')\n",
    "    plt.title(f'Actual fault-{F_num}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize diffrent types of faults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "logreg = joblib.load('model/logistic_regression_model.pkl')\n",
    "dt = joblib.load('model/decision_tree_model.pkl')\n",
    "rf = joblib.load('model/random_forest_model.pkl')\n",
    "nb = joblib.load('model/naive_bayes_model.pkl')\n",
    "knn = joblib.load('model/knn_model.pkl')\n",
    "xg = joblib.load('model/xgboost_model.pkl')\n",
    "sc = joblib.load('model/standard_scalar.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from explainerdashboard import ClassifierExplainer, ExplainerDashboard\n",
    "from explainerdashboard.datasets import titanic_embarked, feature_descriptions\n",
    "feature_descriptions\n",
    "type(le.classes_)\n",
    "ls = list(set(y_test))\n",
    "ls1 = [str(n) for n in ls] \n",
    "ls1\n",
    "explainer = ClassifierExplainer(xg, x_test, y_test,  \n",
    "                                descriptions=X_dict,\n",
    "                                labels=ls1,\n",
    "                                pos_label=ls1[0]\n",
    "                                )\n",
    "\n",
    "ExplainerDashboard(explainer).run()\n",
    "4 * np.random.rand(20) - 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
